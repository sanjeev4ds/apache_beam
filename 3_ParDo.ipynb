{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1XXcP2CxjdI",
        "outputId": "fa64d27a-55e2-42b0-f360-450b5ddef5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m793.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires grpcio>=1.71.0, but you have grpcio 1.65.5 which is incompatible.\n",
            "distributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "dask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!{'pip install --quiet apache_beam'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "class Filter_Dept(beam.DoFn):\n",
        "  def process(self, element, dept):\n",
        "    if element[3]==dept:\n",
        "      return [element]\n",
        "\n",
        "class Split_Row(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    return [element.split(',')]\n",
        "\n",
        "class PairWithOne(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    return [(element[3] + \", \"+ element[1], 1)]\n",
        "\n",
        "class SumAllValues(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    key, values = element\n",
        "    return [(key, sum(values))]\n",
        "\n",
        "with beam.Pipeline() as p:\n",
        "  input_collection = (\n",
        "      p\n",
        "      | 'read from text file' >> beam.io.ReadFromText('drive/MyDrive/APACHE_BEAM/datasets/dept_data.txt')\n",
        "      | 'split rows' >> beam.ParDo(Split_Row())\n",
        "  )\n",
        "  accounts_count = (\n",
        "      input_collection\n",
        "      | 'Get Accounts dept records' >> beam.ParDo(Filter_Dept(), \"Accounts\")\n",
        "                                      #beam.Filter(lambda record: record[3]==\"Accounts\")\n",
        "      | 'Pair Account record with 1' >> beam.ParDo(PairWithOne())\n",
        "\n",
        "      | 'Group_Accounts' >> beam.GroupByKey()\n",
        "      | 'Sum_Accounts' >> beam.ParDo(SumAllValues())\n",
        "      # | 'group and sum Account' >> beam.CombinePerKey(sum)\n",
        "\n",
        "      # | 'Write results for account' >> beam.io.Write('data/Account')\n",
        "  )\n",
        "\n",
        "  hr_count = (\n",
        "      input_collection\n",
        "      | 'Get HR dept records' >> beam.ParDo(Filter_Dept(), \"HR\")\n",
        "                                  #beam.Filter(lambda record: record[3]==\"HR\")\n",
        "      | 'Pair HR record with 1' >> beam.ParDo(PairWithOne())\n",
        "\n",
        "      | 'Group_HR' >> beam.GroupByKey()\n",
        "      | 'Sum_HR' >> beam.ParDo(SumAllValues())\n",
        "      # | 'group and sum HR' >> beam.CombinePerKey(sum)\n",
        "\n",
        "      # | 'Write results for account' >> beam.io.Write('data/Account')\n",
        "  )\n",
        "\n",
        "  final_count = (\n",
        "      (accounts_count, hr_count)\n",
        "      | beam.Flatten()\n",
        "      |beam.Map(print)\n",
        "      # | beam.io.WriteToText(\"drive/MyDrive/APACHE_BEAM/output/final_data.txt\")\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CydftDc7xzVZ",
        "outputId": "d9b21bf5-1ed3-44db-fb57-fe823c594fc0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Accounts, Marco', 31)\n",
            "('Accounts, Rebekah', 31)\n",
            "('Accounts, Itoe', 31)\n",
            "('Accounts, Edouard', 31)\n",
            "('Accounts, Kyle', 62)\n",
            "('Accounts, Kumiko', 31)\n",
            "('Accounts, Gaston', 31)\n",
            "('Accounts, Ayumi', 30)\n",
            "('HR, Beryl', 62)\n",
            "('HR, Olga', 31)\n",
            "('HR, Leslie', 31)\n",
            "('HR, Mindy', 31)\n",
            "('HR, Vicky', 31)\n",
            "('HR, Richard', 31)\n",
            "('HR, Kirk', 31)\n",
            "('HR, Kaori', 31)\n",
            "('HR, Oscar', 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9G6XLNoUzn_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}